import itertools as it
import logging
import os
import random
import time
import uuid
from datetime import datetime
from enum import Enum
from typing import Any, Dict, Iterable, List, Optional, Tuple, Union, cast, Mapping
from urllib.parse import urlparse

import boto3  # type: ignore
import pytz
import requests
from botocore.exceptions import ClientError  # type: ignore
from fake_useragent import UserAgent  # type: ignore

from ..models import Listing, Property, Source

logger = logging.getLogger(__name__)


class Transaction(Enum):
    rent = 10
    buy = 9


class PropertyType(Enum):
    house = 1
    apartment = 2
    land = 3
    parking = 4
    other = 5


def leboncoin(
    transaction: str,
    post_codes: Union[str, Iterable[str]],
    property_types: Union[str, Iterable[str]] = ["apartment", "house"],
    min_price: Optional[float] = None,
    max_price: Optional[float] = None,
    min_size: Optional[float] = None,
    max_size: Optional[float] = None,
    min_rooms: Optional[float] = None,
    max_rooms: Optional[float] = None,
    min_beds: Optional[float] = None,
    max_beds: Optional[float] = None,
    num_results: int = 100,
    max_duplicates: int = 25,
    timeout: int = 5,
) -> Dict[str, Union[List[str], List[Listing]]]:
    from .. import db

    allowed_transactions = cast(Iterable[str], Transaction._member_names_)
    if transaction not in allowed_transactions:
        msg = (
            f"Unknown transaction '{transaction}'. Expected one of "
            f"{', '.join(allowed_transactions)}"
        )
        raise ValueError(msg)
    transaction = cast(str, Transaction[transaction].value)

    if isinstance(post_codes, str):
        post_codes = [post_codes]

    if isinstance(property_types, str):
        property_types = [property_types]
    property_types = [p.lower() for p in property_types]
    allowed_property_types = cast(Iterable[str], PropertyType._member_names_)
    for property_type in property_types:
        if property_type not in allowed_property_types:
            msg = (
                f"Unknown property_type '{property_type}'. Expected one of "
                f"{', '.join(allowed_property_types)}"
            )
            raise ValueError(msg)
    property_types = [
        PropertyType[property_type].value for property_type in property_types
    ]

    # fetch all the listings already processed
    already_done_urls = [
        l[0]
        for l in db.session.query(Listing.url)
        .join(Source)
        .filter(Source.name == "leboncoin")
        .all()
    ]

    # build the search payload
    rooms = {}
    if min_rooms is not None:
        rooms.update({"min": min_rooms})
    if max_rooms is not None:
        rooms.update({"max": max_rooms})
    size = {}
    if min_size is not None:
        size.update({"min": min_size})
    if max_size is not None:
        size.update({"max": max_size})
    price = {}
    if min_price is not None:
        price.update({"min": min_price})
    if max_price is not None:
        price.update({"max": max_price})
    payload = {
        "pivot": "0,0,0",  # page cursor
        "limit": 100,  # number of results par page (100 is server-side max)
        "limit_alu": 1,  # 0 to return only statistics, 1 to also return listings
        "filters": {
            "category": {"id": str(transaction)},
            "enums": {
                "ad_type": ["offer"],
                "real_estate_type": [str(pt) for pt in property_types],
            },
            "ranges": {"rooms": rooms, "square": size, "price": price},
            "location": {
                "locations": [
                    {"locationType": "city", "zipcode": post_code}
                    for post_code in post_codes
                ]
            },
        },
        "sort_by": "time",
        "sort_order": "desc",
    }

    # user agent generator
    ua = UserAgent()

    # get a list of proxies
    proxy_list = requests.get(
        "https://www.proxy-list.download/api/v1/get", params={"type": "https"}
    ).text.split()
    random.shuffle(proxy_list)
    proxy_pool = it.cycle(proxy_list)
    proxy = next(proxy_pool)

    # post the query
    search_url = "https://api.leboncoin.fr/api/adfinder/v1/search"
    headers = {
        "User-Agent": ua.random,
        "Accept-Encoding": "gzip, deflate",
        "Accept-Language": "en-US,en;q=0.8,fr;q=0.6",
        "Referer": "https://www.leboncoin.fr/recherche",
        "Origin": "https://www.leboncoin.fr",
    }
    done_with_all_pages = False
    added_listings: List[Listing] = []
    seen_listings: List[Listing] = []
    failed_listings: List[str] = []
    while not done_with_all_pages:

        search_attempts = 0
        while search_attempts < len(proxy_list):
            proxies = {"http": proxy, "https": proxy}

            try:
                request = requests.post(
                    search_url,
                    headers=headers,
                    json=payload,
                    proxies=proxies,
                    timeout=timeout,
                )
            except requests.exceptions.RequestException as e:
                msg = f"ðŸ‘»Failed to retrieve the page ({type(e).__name__}).ðŸ‘»"
                logger.debug(msg)
                proxy = next(proxy_pool)
                headers.update({"User-Agent": ua.random})
                search_attempts += 1
                continue
            if (
                ("captcha" in request.text)
                or ("datadome" in request.text)
                or (not request.text)
                or (request.status_code >= 400)
            ):
                msg = f"ðŸ‘»Failed to retrieve the page (Captcha).ðŸ‘»"
                logger.debug(msg)
                proxy = next(proxy_pool)
                headers.update({"User-Agent": ua.random})
                search_attempts += 1
                continue
            break
        response = request.json()

        # parse json
        done = -1
        consecutive_duplicates = 0
        for i, ad in enumerate(response.get("ads", [])):
            done += 1
            url = ad.get("url")
            if url in already_done_urls:
                msg = f"Skipping ad #{i}, as it is already in our DB: {url}."
                logger.debug(msg)
                consecutive_duplicates += 1
                seen_listings.append(url)
                continue

            msg = f"Parsing ad #{i}: {url} ..."
            logger.debug(msg)
            try:
                listing, is_new = _leboncoin(ad, headers, proxies)
            except Exception:
                msg = f"ðŸ’¥Unpexpected error.ðŸ’¥"
                logging.exception(msg)
                failed_listings.append(url)
                continue
            msg = f"ðŸ’«Scrape suceeded.ðŸ’«"
            logger.debug(msg)

            if is_new:
                added_listings.append(listing)
                consecutive_duplicates = 0
            else:
                seen_listings.append(listing)
                consecutive_duplicates += 1

        if (
            ("pivot" in response)
            and (consecutive_duplicates <= max_duplicates)
            and (done <= num_results)
        ):
            sleep = random.randint(5, 35)
            msg = f"Sleeping for {sleep} seconds before going to next page."
            logger.debug(msg)
            time.sleep(sleep)
            payload.update({"pivot": response["pivot"]})
            # NB: we keep the same proxy and user agent
        else:
            done_with_all_pages = True

    return {"added": added_listings, "seen": seen_listings, "failed": failed_listings}


def _leboncoin(
    ad: Mapping[str, Any], headers: Mapping[str, str], proxies: Mapping[str, str]
) -> Tuple[Listing, bool]:

    from .. import db

    fields: Dict[str, str] = {
        "external_listing_id": "list_id",
        "first_publication_date": "first_publication_date",
        "description": "body",
        "url": "url",
        "transaction": "category_name",
        "price": "price",
    }
    data: Dict[str, Any] = {field: ad.get(fields[field]) for field in fields}
    assert isinstance(data["price"], list)
    assert len(data["price"]) == 1
    data["price"] = data["price"][0]
    data["external_listing_id"] = str(data["external_listing_id"])
    assert isinstance(data["first_publication_date"], str)
    data["first_publication_date"] = (
        pytz.timezone("Europe/Paris")
        .localize(datetime.fromisoformat(data["first_publication_date"]))
        .astimezone(pytz.utc)
        .isoformat()
    )

    attributes_fields = {
        "property_type": ("real_estate_type", "value_label"),
        "size": ("square", "value"),
        "rooms": ("rooms", "value"),
        "is_furnished": ("furnished", "value"),
        "charges_included": ("charges_included", "value"),
        "broker_fee": ("fai", "value"),
        "broker_fee_is_included": ("fai_included", "value"),
        "dpe_consumption": ("energy_rate", "value"),
        "dpe_emissions": ("ges", "value"),
    }

    def _get_attribute_field(
        ad: Mapping[str, Any], key: str, value_field: str
    ) -> Optional[str]:
        _dict = [d for d in ad["attributes"] if d["key"] == key]
        if not _dict:
            return None
        if len(_dict) > 1:
            msg = f"Unexpectedly got more than one match for attribute '{key}'."
            raise RuntimeError(msg)

        return _dict[0][value_field]

    data.update(
        {
            field: _get_attribute_field(ad, *attributes_fields[field])
            for field in attributes_fields
        }
    )
    if "is_furnished" in data:
        data["is_furnished"] = data["is_furnished"] == "1"
    if "broker_fee_is_included" in data:
        data["broker_fee_is_included"] = data["broker_fee_is_included"] == "1"
    if "dpe_consumption" in data:
        data["dpe_consumption"] = (
            data["dpe_consumption"] if data["dpe_consumption"] != "v" else None
        )
    if "dpe_emissions" in data:
        data["dpe_emissions"] = (
            data["dpe_emissions"] if data["dpe_emissions"] != "v" else None
        )

    if data.get("charges_included", "1") not in ("1", None):
        msg = f"Charges are not included: {ad}"
        raise NotImplementedError(msg)

    null_values = ["non renseignÃ©"]
    data = {
        k: data[k] if str(data[k]).lower() not in null_values else None for k in data
    }

    location_fields = {
        "city": "city",
        "postal_code": "zipcode",
        "neighborhood": "city_label",
        "latitude": "lat",
        "longitude": "lng",
    }
    data.update(
        {
            field: ad.get("location", {}).get(location_fields[field])
            for field in location_fields
        }
    )

    for field in ["size", "rooms", "broker_fee"]:
        if (field not in data) or (not isinstance(data[field], str)):
            continue
        try:
            data[field] = float(data[field].replace(",", "."))
        except (KeyError, ValueError, AttributeError):
            data[field] = None

    # download the images
    s3 = boto3.client("s3")
    bucket = os.getenv("BUCKET_NAME")
    folder = uuid.uuid4()
    remote_image_urls = ad.get("images", {}).get("urls", [])
    n_images = len(remote_image_urls)
    local_image_paths: List[Optional[str]] = [None] * n_images
    width = max(len(str(n_images)), 2)
    for i, remote_image_url in enumerate(remote_image_urls):
        retries = 3
        while True:
            try:
                http_response = requests.get(
                    remote_image_url, headers=headers, proxies=proxies
                )
                break
            except requests.exceptions.RequestException:
                time.sleep(5)
                if retries <= 0:
                    msg = f"Could not download image #{i}."
                    logger.warning(msg)
                    break
                retries -= 1
        if retries <= 0:
            continue

        name = str(i + 1).zfill(width)
        _, extension = os.path.splitext(urlparse(remote_image_url).path)
        local_image_path = f"leboncoin/{folder}/{name}{extension}"
        local_image_paths[i] = local_image_path
        try:
            s3.put_object(
                Body=http_response.content, Bucket=bucket, Key=local_image_path
            )
        except ClientError:
            msg = f"Could not download image #{i}."
            logger.exception(msg)
            continue

    data["images"] = list(filter(None, local_image_paths)) or None

    data["source"] = "leboncoin"

    property = Property.create(data)
    db.session.add(property)
    db.session.flush()
    data.update({"property_id": property.id})
    listing, is_new = Listing.get_or_create(**data)
    if is_new:
        db.session.add(listing)
        db.session.commit()

    return listing, is_new
